# Activation Functions

![Activation_Functions](doc/Activation_Functions.png)

### Sigmoid Function

![Sigmoid_Function](doc/Sigmoid_Function.png)

Function:

<p align="center"><img src="tex/9b171bd87aa286bf84d6621ea1204017.svg?invert_in_darkmode" align=middle width=113.14248pt height=34.3600389pt/></p>

Derivative:

<p align="center"><img src="tex/1a3902d66dffcc33134633eb13a56e4a.svg?invert_in_darkmode" align=middle width=187.36269915pt height=17.2895712pt/></p>

### Tanh Function

![Tanh_Function.png](doc/Tanh_Function.png)

Function:

<p align="center"><img src="tex/7eb4be07a0429a57780410969ed58d1a.svg?invert_in_darkmode" align=middle width=197.82854354999998pt height=36.8550864pt/></p>

Derivative:

<p align="center"><img src="tex/2d95939262cdc426890def2845d69e00.svg?invert_in_darkmode" align=middle width=128.12775195pt height=18.312383099999998pt/></p>

### Rectified Linear Unit (ReLU)

![Rectified_Linear_Unit_(ReLU)](doc/Rectified_Linear_Unit.png)

Function:

<p align="center"><img src="tex/83803c6cf357e7afb8cdabf1e530ea97.svg?invert_in_darkmode" align=middle width=154.89708299999998pt height=49.315569599999996pt/></p>

Derivative:

<p align="center"><img src="tex/2172629849e5868eaf600934f256c186.svg?invert_in_darkmode" align=middle width=220.89037574999998pt height=69.0417981pt/></p>

### Leaky ReLU

![Leaky_ReLU](doc/Leaky_ReLU.png)

Function:

<p align="center"><img src="tex/a4bbb3b4859a057a266b6c31e636abc7.svg?invert_in_darkmode" align=middle width=184.12091775pt height=49.315569599999996pt/></p>

Derivative:

<p align="center"><img src="tex/a5d8a53e48a44e595830cd70188848a5.svg?invert_in_darkmode" align=middle width=179.3378268pt height=49.315569599999996pt/></p>

### Parametric ReLU

![Parameteric_ReLU](doc/Parameteric_ReLU.png)

Function:

<p align="center"><img src="tex/290ecca72cd3c083c37a6bdff5f8d689.svg?invert_in_darkmode" align=middle width=165.47358135pt height=49.315569599999996pt/></p>

Derivative:

<p align="center"><img src="tex/754c5b79c77621fd1c89885a39b8d291.svg?invert_in_darkmode" align=middle width=160.6904904pt height=49.315569599999996pt/></p>

### Exponential Linear Unit (ELU)

![Exponential_Linear_Unit_(ELU)](doc/Exponential_Linear_Unit.png)

Function:

<p align="center"><img src="tex/22b919815535e3da79a74831f137d534.svg?invert_in_darkmode" align=middle width=267.89914304999996pt height=49.315569599999996pt/></p>

Derivative:

<p align="center"><img src="tex/e859654ddf616a4d426f9a15ef699144.svg?invert_in_darkmode" align=middle width=256.1470461pt height=69.0417981pt/></p>

### Scaled Exponential Linear Unit (SELU)

![Scaled_Exponential_Linear_Unit_(SELU)](doc/Scaled_Exponential_Linear_Unit.png)

Function:

<p align="center"><img src="tex/822646f49afad2437610e66ee730bef7.svg?invert_in_darkmode" align=middle width=663.27940635pt height=49.315569599999996pt/></p>

Derivative:

<p align="center"><img src="tex/3e743f8c72715fac3f04a831660936ed.svg?invert_in_darkmode" align=middle width=188.9496345pt height=49.315569599999996pt/></p>

### Gaussian Error Linear Unit (GELU)

![Gaussian_Error_Linear_Unit_(GELU)](doc/Gaussian_Error_Linear_Unit.png)

Function:

<p align="center"><img src="tex/5e156666e8767505b7fdc17f061898f7.svg?invert_in_darkmode" align=middle width=344.55862485pt height=39.452455349999994pt/></p>

Derivative:

<p align="center"><img src="tex/7e28db664ad627340f7fda25a290ac36.svg?invert_in_darkmode" align=middle width=162.26000175pt height=17.2895712pt/></p>

### Sigmoid-Weighted Linear Unit (SiLU) / Swish

![Sigmoid_Weighted_Linear_Unit_(SiLU)_Swish](doc/Sigmoid_Weighted_Linear_Unit_Swish.png)

Function:

<p align="center"><img src="tex/a7441e8a4f2fdb45cfc82da527cbafed.svg?invert_in_darkmode" align=middle width=113.14248pt height=30.8440539pt/></p>

Derivative:

<p align="center"><img src="tex/d0f701c20d414f274f5a81ef8eb6be5c.svg?invert_in_darkmode" align=middle width=188.96986679999998pt height=42.190962pt/></p>

### Softmax Function

Function:

<p align="center"><img src="tex/789a1f0365c3e83c7d1dc4a8b10d0acf.svg?invert_in_darkmode" align=middle width=124.36085145pt height=43.5290361pt/></p>

### Softplus

![SoftPlus](doc/SoftPlus.png)

Function:

<p align="center"><img src="tex/57007cfe55ba83df3eeedbdc9d6485b4.svg?invert_in_darkmode" align=middle width=135.59910539999998pt height=16.438356pt/></p>

Derivative:

<p align="center"><img src="tex/b5f56261f1d93afbbe17f2cba27d68d9.svg?invert_in_darkmode" align=middle width=117.7543554pt height=34.3600389pt/></p>

### Mish

![Mish_Function](doc/Mish_Function.png)

Function:

<p align="center"><img src="tex/80c7c3a438606431b27cc86bce2f0135.svg?invert_in_darkmode" align=middle width=190.6561932pt height=16.438356pt/></p>

Derivative:

<p align="center"><img src="tex/b12dfd5e9d8bfe92d02c115de29172d4.svg?invert_in_darkmode" align=middle width=364.33511895pt height=44.11870485pt/></p>

## Resources

- [https://en.wikipedia.org/wiki/Activation_function](https://en.wikipedia.org/wiki/Activation_function)
- [https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)
- [https://www.v7labs.com/blog/neural-networks-activation-functions](https://www.v7labs.com/blog/neural-networks-activation-functions)
- [https://mlfromscratch.com/activation-functions-explained/#/](https://mlfromscratch.com/activation-functions-explained/#/)
- [https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html](https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html)