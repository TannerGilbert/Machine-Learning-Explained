# Machine-Learning-Explained

This repository contains explanations and implementations of machine learning algorithms and concepts. The explanations are also available as articles on [my website](https://ml-explained.com/).

## Machine Learning Algorithms

* [Linear Regression](Algorithms/linear_regression)
* [Logistic Regression](Algorithms/logistic_regression)
* [K Nearest Neighbors](Algorithms/k_nearest_neighbors)
* [Decision Tree](Algorithms/decision_tree)
* [KMeans](Algorithms/kmeans)
* [Mean Shift](Algorithms/mean_shift)
* [DBSCAN](Algorithms/dbscan)
* [Random Forest](Algorithms/random_forest)
* [Adaboost](Algorithms/adaboost)
* [Gradient Boosting](Algorithms/gradient_boosting)
* [Principal Component Analysis (PCA)](Algorithms/principal_component_analysis)

## Optimizers

* [Gradient Descent](Optimizers/gradient_descent)
* [Adagrad](Optimizers/adagrad)
* [Adadelta](Optimizers/adadelta)
* [RMSprop](Optimizers/rmsprop)
* [Adam](Optimizers/adam)
* [AdaMax](Optimizers/adamax)
* [Nadam](Optimizers/nadam)
* [AMSGrad](Optimizers/amsgrad)
* [AdamW](Optimizers/adamw)
* [QHAdam](Optimizers/qhadam)

## Activation Functions

* [ELU](Activation_Functions/elu)
* [Leaky RELU](Activation_Functions/leaky_relu)
* [RELU](Activation_Functions/relu)
* [SELU](Activation_Functions/selu)
* [Sigmoid](Activation_Functions/sigmoid)
* [SILU](Activation_Functions/silu)
* [Softmax](Activation_Functions/softmax)
* [Softplus](Activation_Functions/softplus)
* [Tanh](Activation_Functions/tanh)

## Loss Functions

* [Cross Entropy](Loss_Functions/cross_entropy)
* [Hinge Loss](Loss_Functions/hinge_loss)
* [Mean absolute error](Loss_Functions/mean_absolute_error)
* [Mean squared error](Loss_Functions/mean_squared_error)

## Contributing

Contributions to Machine-Learning-Explained are always welcome, whether code or documentation changes. For contribution guidelines, please see the [CONTRIBUTING.md file](CONTRIBUTING.md).

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE) file for details.